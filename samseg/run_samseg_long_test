#!/usr/bin/env python
#%%
import sys
sys.path.append("/scratch/users/lhaupt/ext/freesurfer_build/freesurfer/python/packages")
import os
import argparse
import surfa as sf
import freesurfer.samseg as samseg
import debugpy




#debugpy.listen(54313)
#debugpy.wait_for_client()

# ------ Parse Command Line Arguments ------

# ------ Initial Setup ------

# Make sure freesurfer has been sourced
if not os.environ.get('FREESURFER_HOME'):
    sf.system.fatal('FREESURFER_HOME must be set')
#%%
parser = argparse.ArgumentParser()

default_threads = int(os.environ.get('OMP_NUM_THREADS', 1))

# required
parser.add_argument('-t', '--timepoint', nargs='+', action='append', required=True, help='Configure a timepoint with multiple inputs.')
parser.add_argument('-o', '--output', required=True, help='Output directory.')
# optional lesion options
parser.add_argument('--lesion', action='store_true', default=False, help='Enable lesion segmentation (requires tensorflow).')
parser.add_argument('--threshold', type=float, default=0.3, help='Lesion threshold for final segmentation. Lesion segmentation must be enabled.')
parser.add_argument('--samples', type=int, default=50, help='Number of samples for lesion segmentation. Lesion segmentation must be enabled.')
parser.add_argument('--burnin', type=int, default=50, help='Number of burn-in samples for lesion segmentation. Lesion segmentation must be enabled.')
parser.add_argument('--lesion-mask-structure', default='Cortex', help='Intensity mask brain structure. Lesion segmentation must be enabled.')
parser.add_argument('--lesion-mask-pattern', type=int, nargs='+', help='Lesion mask list (set value for each input volume): -1 below lesion mask structure mean, +1 above, 0 no mask. Lesion segmentation must be enabled.')
# optional processing options
parser.add_argument('-m', '--mode', nargs='+', help='Output basenames for the input image mode.')
parser.add_argument('-a', '--atlas', metavar='DIR', help='Point to an alternative atlas directory.')
parser.add_argument('--deformation-hyperprior', type=float, default=20.0, help='Strength of the latent deformation hyperprior.')
parser.add_argument('--gmm-hyperprior', type=float, default=0.5, help='Strength of the latent GMM hyperprior.')
parser.add_argument('--save-warp', action='store_true', help='Save the image->template warp fields.')
parser.add_argument('--save-mesh', action='store_true', help='Save the final mesh of each timepoint in template space.')
parser.add_argument('--save-posteriors', nargs='*', help='Save posterior volumes to the "posteriors" subdirectory.')
parser.add_argument('--pallidum-separate', action='store_true', default=False, help='Move pallidum outside of global white matter class. Use this flag when T2/flair is used.')
parser.add_argument('--threads', type=int, default=default_threads, help='Number of threads to use. Defaults to current OMP_NUM_THREADS or 1.')
# optional debugging options
parser.add_argument('--history', action='store_true', default=False, help='Save history.')
parser.add_argument('--showfigs', action='store_true', default=False, help='Show figures during run.')
parser.add_argument('--movie', action='store_true', default=False, help='Show history as arrow key controlled time sequence.')

args = parser.parse_args(["-t", "/scratch/users/lhaupt/data/NCANDA/MRIs/NCANDA_S00033_baseline_brain.nii.gz", \
                    "-t", "/scratch/users/lhaupt/data/NCANDA/MRIs/NCANDA_S00033_followup_1y_brain.nii.gz", \
                    "-t", "/scratch/users/lhaupt/data/NCANDA/MRIs/NCANDA_S00033_followup_2y_brain.nii.gz", 
                    "-t", "/scratch/users/lhaupt/data/NCANDA/MRIs/NCANDA_S00033_followup_3y_brain.nii.gz",
                    "-o", "/scratch/users/lhaupt/data/NCANDA/lables_samseg3"])

#%%
import os
import numpy as np
import pickle

from freesurfer.samseg.SamsegUtility import *
from freesurfer.samseg.utilities import requireNumpyArray
from freesurfer.samseg.figures import initVisualizer
from freesurfer.samseg.Affine import Affine
from freesurfer.samseg.ProbabilisticAtlas import ProbabilisticAtlas
from freesurfer.samseg.Samseg import Samseg
from freesurfer.samseg.SubjectSpecificAtlas import SubjectSpecificAtlas
from freesurfer.samseg import gems
import pdb

#%%
# Make sure more than 1 timepoint was specified
if len(args.timepoint) == 1:
    sf.system.fatal('must provide more than 1 timepoint')

# Make sure freesurfer has been sourced
if not os.environ.get('FREESURFER_HOME'):
    sf.system.fatal('FREESURFER_HOME must be set')

# Create the output folder
os.makedirs(args.output, exist_ok=True)

# Specify the maximum number of threads the GEMS code will use
samseg.setGlobalDefaultNumberOfThreads(args.threads)

# Get the atlas directory
atlasDir = os.environ.get('SAMSEG_DATA_DIR')
if args.atlas:
    atlasDir = args.atlas
if not atlasDir:
    # Atlas defaults
    if args.lesion:
        defaultAtlas = '20Subjects_smoothing2_down2_smoothingForAffine2_lesion'
    else:
        defaultAtlas = '20Subjects_smoothing2_down2_smoothingForAffine2'
    atlasDir = os.path.join(os.environ.get('FREESURFER_HOME'), 'average', 'samseg', defaultAtlas)

# Setup the visualization tool
visualizer = samseg.initVisualizer(args.showfigs, args.movie)

# Start the process timer
timer = samseg.Timer()

# Check if --save-posteriors was specified without any structure search string
if args.save_posteriors is not None and len(args.save_posteriors) == 0:
    savePosteriors = True
else:
    savePosteriors = args.save_posteriors


samseg_kwargs = dict(
    imageFileNamesList=args.timepoint,
    atlasDir=atlasDir,
    imageTimePoints = [1,2,3,4],
    savePath=args.output,
    targetIntensity=110,
    targetSearchStrings=['Cerebral-White-Matter'],
    modeNames=args.mode,
    pallidumAsWM=(not args.pallidum_separate),
    strengthOfLatentDeformationHyperprior=args.deformation_hyperprior,
    strengthOfLatentGMMHyperprior=args.gmm_hyperprior,
    savePosteriors=savePosteriors,
    saveMesh=args.save_mesh,
    saveHistory=args.history,
    visualizer=visualizer
)

samseg_long = samseg.SamsegLongitudinal3(**samseg_kwargs)

#%%

samseg_long.sstModelObject.fitModel()

samseg_long.sstModel = samseg_long.sstModelObject.getSubjectSpecificTemplate()
samseg_long.imageToImageTransformMatrix = samseg_long.sstModelObject.imageToImageTransformMatrix
samseg_long.imageBuffersList = samseg_long.sstModelObject.imageBuffersList
if samseg_long.saveHistory:
    samseg_long.history.update(samseg_long.sstModelObject.history)
samseg_long.preProcess()
samseg_long.fitModel()
#%%

samseg_long.segment()
# %%

samseg_long.latentDeformation = [None] * samseg_long.numberOfTimepoints
samseg_long.latentDeformationAtlasFileName = [None] * samseg_long.numberOfTimepoints
samseg_long.latentMeans = [None] * samseg_long.numberOfTimepoints
samseg_long.latentVariances = [None] * samseg_long.numberOfTimepoints
samseg_long.latentMixtureWeights = [None] * samseg_long.numberOfTimepoints
samseg_long.latentAtlasFileName = [None] * samseg_long.numberOfTimepoints
for timepointNumber in range(samseg_long.numberOfTimepoints):
    samseg_long.timepointModels[timepointNumber].initializeGMM()
    samseg_long.timepointModels[timepointNumber].gmm.means = samseg_long.sstModelObject.sstModel.gmm.means.copy()
    samseg_long.timepointModels[timepointNumber].gmm.variances = samseg_long.sstModelObject.sstModel.gmm.variances.copy()
    samseg_long.timepointModels[timepointNumber].gmm.mixtureWeights = samseg_long.sstModelObject.sstModel.gmm.mixtureWeights.copy()
    samseg_long.timepointModels[timepointNumber].initializeBiasField()

    # Initialization of the latent variables, acting as hyperparameters when viewed from the model parameters' perspective
    
    samseg_long.latentDeformation[timepointNumber] = samseg_long.sstModelObject.sstModel.deformation.copy()
    samseg_long.latentDeformationAtlasFileName[timepointNumber] = samseg_long.sstModelObject.sstModel.deformationAtlasFileName
    samseg_long.latentMeans[timepointNumber] = samseg_long.sstModelObject.sstModel.gmm.means.copy()
    samseg_long.latentVariances[timepointNumber] = samseg_long.sstModelObject.sstModel.gmm.variances.copy()
    samseg_long.latentMixtureWeights[timepointNumber] = samseg_long.sstModelObject.sstModel.gmm.mixtureWeights.copy()

K0 = samseg_long.sstModelObject.sstModel.modelSpecifications.K  # Stiffness population -> latent position
K1 = samseg_long.strengthOfLatentDeformationHyperprior * K0  # Stiffness latent position -> each time point
sstEstimatedNumberOfVoxelsPerGaussian = np.sum(samseg_long.sstModelObject.sstModel.optimizationHistory[-1]['posteriorsAtEnd'], axis=0) * \
                                        np.prod(samseg_long.sstModelObject.sstModel.optimizationHistory[-1]['downSamplingFactors'])
numberOfClasses = len(samseg_long.sstModelObject.sstModel.gmm.numberOfGaussiansPerClass)
numberOfGaussians = sum(samseg_long.sstModelObject.sstModel.gmm.numberOfGaussiansPerClass)
samseg_long.latentMeansNumberOfMeasurements = np.zeros(numberOfGaussians)
samseg_long.latentVariancesNumberOfMeasurements = np.zeros(numberOfGaussians)
samseg_long.latentMixtureWeightsNumberOfMeasurements = np.zeros(numberOfClasses)

for classNumber in range(numberOfClasses):
    #
    numberOfComponents = samseg_long.sstModelObject.sstModel.gmm.numberOfGaussiansPerClass[classNumber]
    gaussianNumbers = np.array(np.sum(samseg_long.sstModelObject.sstModel.gmm.numberOfGaussiansPerClass[:classNumber]) +
                                np.array(range(numberOfComponents)), dtype=np.uint32)
    sstEstimatedNumberOfVoxelsInClass = np.sum(sstEstimatedNumberOfVoxelsPerGaussian[gaussianNumbers])

    samseg_long.latentMixtureWeightsNumberOfMeasurements[
        classNumber] = samseg_long.strengthOfLatentGMMHyperprior * sstEstimatedNumberOfVoxelsInClass

    averageSizeOfComponents = sstEstimatedNumberOfVoxelsInClass / numberOfComponents
    samseg_long.latentMeansNumberOfMeasurements[gaussianNumbers] = samseg_long.strengthOfLatentGMMHyperprior * averageSizeOfComponents
    samseg_long.latentVariancesNumberOfMeasurements[gaussianNumbers] = samseg_long.strengthOfLatentGMMHyperprior * averageSizeOfComponents

# Estimating the mode of the latentVariance posterior distribution (which is Wishart) requires a stringent condition
# on latentVariancesNumberOfMeasurements so that the mode is actually defined
threshold = (samseg_long.sstModelObject.sstModel.gmm.numberOfContrasts + 2) + 1e-6
samseg_long.latentVariancesNumberOfMeasurements[samseg_long.latentVariancesNumberOfMeasurements < threshold] = threshold

# No point in updating latent GMM parameters if the GMM hyperprior has zero weight. The latent variances are also
# a bit tricky, as they're technically driven to zero in that scenario -- let's try not to go there...
if samseg_long.strengthOfLatentGMMHyperprior == 0:
    samseg_long.updateLatentMeans, samseg_long.updateLatentVariances, samseg_long.updateLatentMixtureWeights = False, False, False

# Loop over all iterations
samseg_long.historyOfTotalCost, samseg_long.historyOfTotalTimepointCost, samseg_long.historyOfLatentAtlasCost = [], [], []
progressPlot = None
iterationNumber = 0
if samseg_long.saveHistory:
    samseg_long.history = {**samseg_long.history,
                **{
                    "timepointMeansEvolution": [],
                    "timepointVariancesEvolution": [],
                    "timepointMixtureWeightsEvolution": [],
                    "timepointBiasFieldCoefficientsEvolution": [],
                    "timepointDeformationsEvolution": [],
                    "timepointDeformationAtlasFileNamesEvolution": [],
                    "latentMeansEvolution": [],
                    "latentVariancesEvolution": [],
                    "latentMixtureWeightsEvolution": [],
                    "latentDeformationEvolution": [],
                    "latentDeformationAtlasFileNameEvolution": []
                }
                }

# Make latent atlas directory
latentAtlasDirectory = os.path.join(samseg_long.savePath, 'latentAtlases')
os.makedirs(latentAtlasDirectory, exist_ok=True)

# %%


# =======================================================================================
#
# Update parameters for each time point using the current latent variable estimates
#
# =======================================================================================
totalTimepointCost = 0
for timepointNumber in range(samseg_long.numberOfTimepoints):
    # Create a new atlas that will be the basis to deform the individual time points from
    samseg_long.latentAtlasFileName[timepointNumber] = os.path.join(latentAtlasDirectory, 'latentAtlas_%02d_iteration_%02d.mgz' % (timepointNumber, iterationNumber + 1))
    samseg_long.probabilisticAtlas.saveDeformedAtlas(samseg_long.latentDeformationAtlasFileName[timepointNumber], samseg_long.latentAtlasFileName[timepointNumber], samseg_long.latentDeformation[timepointNumber], True)

    # Only use the last resolution level, and with the newly created atlas as atlas

    samseg_long.timepointModels[timepointNumber].optimizationOptions = samseg_long.sstModel.optimizationOptions
    samseg_long.timepointModels[timepointNumber].optimizationOptions['multiResolutionSpecification'] = [
        samseg_long.timepointModels[timepointNumber].optimizationOptions['multiResolutionSpecification'][-1]]
    samseg_long.timepointModels[timepointNumber].optimizationOptions['multiResolutionSpecification'][0]['atlasFileName'] = samseg_long.latentAtlasFileName[timepointNumber]
    print(samseg_long.timepointModels[timepointNumber].optimizationOptions)

    # Loop over all time points

    # TODO add the specific weights here
    samseg_long.timepointModels[timepointNumber].modelSpecifications.K = K1 
    samseg_long.timepointModels[timepointNumber].gmm.hyperMeans = samseg_long.latentMeans[timepointNumber]
    samseg_long.timepointModels[timepointNumber].gmm.hyperVariances = samseg_long.latentVariances[timepointNumber]
    samseg_long.timepointModels[timepointNumber].gmm.hyperMixtureWeights = samseg_long.latentMixtureWeights[timepointNumber]
    samseg_long.timepointModels[timepointNumber].gmm.fullHyperMeansNumberOfMeasurements = samseg_long.latentMeansNumberOfMeasurements.copy()
    samseg_long.timepointModels[timepointNumber].gmm.fullHyperVariancesNumberOfMeasurements = samseg_long.latentVariancesNumberOfMeasurements.copy()
    samseg_long.timepointModels[timepointNumber].gmm.fullHyperMixtureWeightsNumberOfMeasurements = samseg_long.latentMixtureWeightsNumberOfMeasurements.copy()
    # This is where the optimization is done
    samseg_long.timepointModels[timepointNumber].estimateModelParameters(
                                initialBiasFieldCoefficients=samseg_long.timepointModels[timepointNumber].biasField.coefficients,
                                initialDeformation=samseg_long.timepointModels[timepointNumber].deformation,
                                initialDeformationAtlasFileName=samseg_long.timepointModels[timepointNumber].deformationAtlasFileName,
                                skipBiasFieldParameterEstimationInFirstIteration=False,
                                skipGMMParameterEstimationInFirstIteration=(iterationNumber == 0)
                                )

    totalTimepointCost += samseg_long.timepointModels[timepointNumber].optimizationHistory[-1]['historyOfCost'][-1]

    print('=================================')
    print('\n')
    print('timepointNumber: ', timepointNumber)
    print('perVoxelCost: ', samseg_long.timepointModels[timepointNumber].optimizationSummary[-1]['perVoxelCost'])
    print('\n')
    print('=================================')
    if hasattr(samseg_long.visualizer, 'show_flag'):
        import matplotlib.pyplot as plt  # avoid importing matplotlib by default
        plt.ion()
        samseg_long.timepointModels[timepointNumber].biasField.downSampleBasisFunctions([1, 1, 1])
        timepointBiasFields = samseg_long.timepointModels[timepointNumber].biasField.getBiasFields(samseg_long.sstModel.mask)
        timepointData = samseg_long.imageBuffersList[timepointNumber][samseg_long.sstModel.mask, :] - timepointBiasFields[samseg_long.sstModel.mask, :]
        for contrastNumber in range(samseg_long.sstModel.gmm.numberOfContrasts):
            axs = axsList[contrastNumber]
            ax = axs.ravel()[2 + timepointNumber]
            ax.clear()
            ax.hist(timepointData[:, contrastNumber], bins)
            ax.grid()
            ax.set_title('time point ' + str(timepointNumber))
        plt.draw()

    # End loop over time points

# =======================================================================================
#
# Check for convergence.
# =======================================================================================

# In order to also measure the deformation from the population atlas -> latent position,
# create:
#   (1) a mesh collection with as reference position the population reference position, and as positions
#       the currently estimated time point positions.
#   (2) a mesh with the current latent position
# Note that in (1) we don't need those time positions now, but these will come in handy very soon to
# optimize the latent position
#
# The parameter estimation happens in a (potentially) downsampled image grid, so it's import to work in the same space
# when measuring and updating the latentDeformation
#pdb.set_trace()

#%%
for timepointNumber in range(samseg_long.numberOfTimepoints):
    transformUsedForEstimation = gems.KvlTransform(
        requireNumpyArray(samseg_long.sstModelObject.sstModel.optimizationHistory[-1]['downSampledTransformMatrix']))
    mesh_collection = gems.KvlMeshCollection()
    mesh_collection.read(samseg_long.latentDeformationAtlasFileName[timepointNumber])
    mesh_collection.transform(transformUsedForEstimation)
    referencePosition = mesh_collection.reference_position
    timepointPositions = []

    # TODO add hidden state to mesh_collection
    #pdb.set_trace()
    TimepointForLatentNumbers = [timepointNumber]
    if timepointNumber-1 >= 0:
        TimepointForLatentNumbers.append(timepointNumber-1)
    if timepointNumber+1 < samseg_long.numberOfTimepoints:
        TimepointForLatentNumbers.append(timepointNumber+1)

    for timepointNumberNeighbours in TimepointForLatentNumbers:
        positionInTemplateSpace = samseg_long.probabilisticAtlas.mapPositionsFromSubjectToTemplateSpace(referencePosition,
                                                                        transformUsedForEstimation) + \
                                samseg_long.latentDeformation[timepointNumber] + samseg_long.timepointModels[timepointNumberNeighbours].deformation
        timepointPositions.append(
            samseg_long.probabilisticAtlas.mapPositionsFromTemplateToSubjectSpace(positionInTemplateSpace, transformUsedForEstimation))
    mesh_collection.set_positions(referencePosition, timepointPositions)

    # Read mesh in sst warp
    # TODO: Fix this
    mesh = samseg_long.probabilisticAtlas.getMesh(samseg_long.latentAtlasFileName[timepointNumber], transformUsedForEstimation)

    #
    calculator = gems.KvlCostAndGradientCalculator(mesh_collection, K0, 0.0, transformUsedForEstimation)
    latentAtlasCost, _ = calculator.evaluate_mesh_position(mesh)

    #
    totalCost = totalTimepointCost + latentAtlasCost
    print('*' * 100 + '\n')
    print('iterationNumber: ', iterationNumber)
    print('totalCost: ', totalCost)
    print('   latentAtlasCost: ', latentAtlasCost)
    print('   totalTimepointCost: ', totalTimepointCost)
    print('*' * 100 + '\n')
    samseg_long.historyOfTotalCost.append(totalCost)
    samseg_long.historyOfTotalTimepointCost.append(totalTimepointCost)
    samseg_long.historyOfLatentAtlasCost.append(latentAtlasCost)

    if hasattr(samseg_long.visualizer, 'show_flag'):
        import matplotlib.pyplot as plt  # avoid importing matplotlib by default
        plt.ion()
        if progressPlot is None:
            plt.figure()
            progressPlot = plt.subplot()
        progressPlot.clear()
        progressPlot.plot(samseg_long.historyOfTotalCost, color='k')
        progressPlot.plot(samseg_long.historyOfTotalTimepointCost, linestyle='-.', color='b')
        progressPlot.plot(samseg_long.historyOfLatentAtlasCost, linestyle='-.', color='r')
        progressPlot.grid()
        progressPlot.legend(['total', 'timepoints', 'latent atlas deformation'])
        plt.draw()

    if samseg_long.saveHistory:
        samseg_long.history["timepointMeansEvolution"].append(samseg_long.timepointModels[timepointNumber].gmm.means.copy())
        samseg_long.history["timepointVariancesEvolution"].append(samseg_long.timepointModels[timepointNumber].gmm.variances.copy())
        samseg_long.history["timepointMixtureWeightsEvolution"].append(samseg_long.timepointModels[timepointNumber].gmm.mixtureWeights.copy())
        samseg_long.history["timepointBiasFieldCoefficientsEvolution"].append(samseg_long.timepointModels[timepointNumber].biasField.coefficients.copy())
        samseg_long.history["timepointDeformationsEvolution"].append(samseg_long.timepointModels[timepointNumber].deformation)
        samseg_long.history["timepointDeformationAtlasFileNamesEvolution"].append(samseg_long.timepointModels[timepointNumber].deformationAtlasFileName)
        samseg_long.history["latentMeansEvolution"].append(samseg_long.latentMeans.copy())
        samseg_long.history["latentVariancesEvolution"].append(samseg_long.latentVariances.copy())
        samseg_long.history["latentMixtureWeightsEvolution"].append(samseg_long.latentMixtureWeights.copy())
        samseg_long.history["latentDeformationEvolution"].append(samseg_long.latentDeformation.copy())
        samseg_long.history["latentDeformationAtlasFileNameEvolution"].append(samseg_long.latentDeformationAtlasFileName)

    if iterationNumber >= (samseg_long.numberOfIterations - 1):
        print('Stopping')
        break

    # =======================================================================================
    #
    # Update the latent variables based on the current parameter estimates
    #
    # =======================================================================================
    # mesh_collection - mesh collection of all segmentation
    # mesh  -  mesh of registered probabilistic atlas
    samseg_long.updateLatentDeformationAtlas(mesh_collection, mesh, K0, K1, transformUsedForEstimation, timepointNumber)
    samseg_long.updateGMMLatentVariables(timepointNumber)

    iterationNumber += 1
    # End loop over parameter and latent variable estimation iterations

# %%
for timepointNumber in range(samseg_long.numberOfTimepoints):
#
    timepointModel = samseg_long.timepointModels[timepointNumber]

    #
    timepointModel.deformation = samseg_long.latentDeformation + timepointModel.deformation
    timepointModel.deformation = np.sum(timepointModel.deformation, axis=0)
    timepointModel.deformationAtlasFileName = samseg_long.latentDeformationAtlasFileName[timepointNumber]
    posteriors, biasFields, nodePositions, _, _ = timepointModel.computeFinalSegmentation()

    #
    timepointDir = os.path.join(samseg_long.savePath, 'tp%03d' % (timepointNumber + 1))
    os.makedirs(timepointDir, exist_ok=True)

    #
    timepointModel.savePath = timepointDir
    volumesInCubicMm = timepointModel.writeResults(biasFields, posteriors)

    # Save the timepoint->template warp
    if saveWarp:
        timepointModel.saveWarpField(os.path.join(timepointDir, 'template.m3z'))

    # Save the final mesh collection
    if samseg_long.saveMesh:
        print('Saving the final mesh in template space')
        deformedAtlasFileName = os.path.join(timepointModel.savePath, 'mesh.txt')
        timepointModel.probabilisticAtlas.saveDeformedAtlas(timepointModel.modelSpecifications.atlasFileName,
                                                            deformedAtlasFileName, nodePositions)

    # Save the history of the parameter estimation process
    if samseg_long.saveHistory:
        history = {'input': {
            'imageFileNames': timepointModel.imageFileNames,
            'imageToImageTransformMatrix': timepointModel.imageToImageTransformMatrix,
            'modelSpecifications': timepointModel.modelSpecifications,
            'optimizationOptions': timepointModel.optimizationOptions,
            'savePath': timepointModel.savePath
        }, 'imageBuffers': timepointModel.imageBuffers, 'mask': timepointModel.mask,
            'cropping': timepointModel.cropping,
            'transform': timepointModel.transform.as_numpy_array,
            'historyWithinEachMultiResolutionLevel': timepointModel.optimizationHistory,
            "labels": timepointModel.modelSpecifications.FreeSurferLabels, "names": timepointModel.modelSpecifications.names,
            "volumesInCubicMm": volumesInCubicMm, "optimizationSummary": timepointModel.optimizationSummary}
        with open(os.path.join(timepointModel.savePath, 'history.p'), 'wb') as file:
            pickle.dump(history, file, protocol=pickle.HIGHEST_PROTOCOL)


    samseg_long.timepointVolumesInCubicMm.append(volumesInCubicMm)

    #
    samseg_long.optimizationSummary = {
    "historyOfTotalCost": samseg_long.historyOfTotalCost,
    "historyOfTotalTimepointCost": samseg_long.historyOfTotalTimepointCost,
    "historyOfLatentAtlasCost": samseg_long.historyOfLatentAtlasCost
    }

    if samseg_long.saveHistory:
        samseg_long.history["labels"] = samseg_long.sstModel.modelSpecifications.FreeSurferLabels
        samseg_long.history["names"] = samseg_long.sstModel.modelSpecifications.names
        samseg_long.history["timepointVolumesInCubicMm"] = samseg_long.timepointVolumesInCubicMm
        samseg_long.history["optimizationSummary"] = samseg_long.optimizationSummary
    with open(os.path.join(samseg_long.savePath, 'history.p'), 'wb') as file:
        pickle.dump(samseg_long.history, file, protocol=pickle.HIGHEST_PROTOCOL)

# %%
samseg_long.timepointModels[0].probabilisticAtlas.getMesh(samseg_long.timepointModels[0].modelSpecifications.atlasFileName, samseg_long.timepointModels[0].transform,
                            initialDeformation=samseg_long.timepointModels[0].deformation[0,:,:],
                            initialDeformationMeshCollectionFileName=samseg_long.timepointModels[0].deformationAtlasFileName)

# %%
for timepointNumber in range(samseg_long.numberOfTimepoints):
    samseg_long.timepointModels[timepointNumber].deformation = np.sum(samseg_long.timepointModels[timepointNumber].deformation, axis=0) 
# %%
